#!/usr/bin/env python3
"""
AI Service Module - Xá»­ lÃ½ tÃ¬m kiáº¿m thÃ´ng minh vÃ  Chat AI
TÃ­ch há»£p há»— trá»£ model má»›i nháº¥t tá»« OpenAI, Local AI, etc.
"""

import re
import os
from datetime import datetime
from typing import List, Dict, Optional, Tuple, Any


# Import models - sáº½ Ä‘Æ°á»£c import khi cáº§n Ä‘á»ƒ trÃ¡nh circular import
def get_document_model():
    """Import Document model Ä‘á»™ng Ä‘á»ƒ trÃ¡nh circular import"""
    from models import Document
    return Document


class AIService:
    """Dá»‹ch vá»¥ AI cho tÃ¬m kiáº¿m thÃ´ng minh vÃ  Chat"""

    # ============ SEARCH DOCUMENTS ============

    @staticmethod
    def search_documents(query: str, limit: int = 10) -> List[Dict[str, Any]]:
        """
        TÃ¬m kiáº¿m thÃ´ng minh vÄƒn báº£n - PhiÃªn báº£n cÆ¡ báº£n

        Args:
            query: Tá»« khÃ³a tÃ¬m kiáº¿m
            limit: Sá»‘ káº¿t quáº£ tá»‘i Ä‘a

        Returns:
            Danh sÃ¡ch tÃ i liá»‡u khá»›p vá»›i Ä‘iá»ƒm sá»‘
        """
        if not query or not query.strip():
            return []

        try:
            from models import Document

            query_lower = query.lower()
            keywords = query_lower.split()

            documents = Document.query.all()
            results = []

            for doc in documents:
                score = 0
                matches = []

                # TÃ¬m kiáº¿m trong tiÃªu Ä‘á»
                if any(kw in doc.title.lower() for kw in keywords):
                    score += 50

                # TÃ¬m kiáº¿m trong ná»™i dung
                if doc.content:
                    content_lower = doc.content.lower()
                    for kw in keywords:
                        count = content_lower.count(kw)
                        if count > 0:
                            score += count * 10
                            idx = content_lower.find(kw)
                            snippet = doc.content[max(0, idx - 50):min(len(doc.content), idx + 100)]
                            matches.append({
                                'snippet': snippet.strip(),
                                'chunk_index': 0
                            })

                # TÃ¬m kiáº¿m trong metadata
                if doc.metadata:
                    if any(kw in str(doc.metadata).lower() for kw in keywords):
                        score += 20

                # TÃ¬m kiáº¿m trong sá»‘ cÃ´ng vÄƒn
                if doc.document_number and any(kw in doc.document_number.lower() for kw in keywords):
                    score += 30

                if score > 0:
                    results.append({
                        'document': doc,
                        'score': score,
                        'matches': matches
                    })

            results = sorted(results, key=lambda x: x['score'], reverse=True)[:limit]
            return results

        except Exception as e:
            print(f"[ERROR] Search error: {str(e)}")
            return []

    @staticmethod
    def search_documents_enhanced(query: str, limit: int = 10) -> List[Dict[str, Any]]:
        """
        TÃ¬m kiáº¿m nÃ¢ng cao - tÃ¬m kiáº¿m trong ná»™i dung Ä‘áº§y Ä‘á»§ vÃ  file Ä‘Ã­nh kÃ¨m

        Args:
            query: Tá»« khÃ³a tÃ¬m kiáº¿m
            limit: Sá»‘ káº¿t quáº£ tá»‘i Ä‘a

        Returns:
            Danh sÃ¡ch tÃ i liá»‡u vá»›i Ä‘iá»ƒm sá»‘ vÃ  matched content
        """
        if not query or not query.strip():
            return []

        try:
            from models import Document

            query_lower = query.lower()
            keywords = query_lower.split()

            documents = Document.query.all()
            results = []

            for doc in documents:
                score = 0
                matches = []
                highlighted_content = ""

                # 1. TÃ¬m kiáº¿m trong tiÃªu Ä‘á» (cao nháº¥t)
                if any(kw in doc.title.lower() for kw in keywords):
                    score += 100

                # 2. TÃ¬m kiáº¿m trong ná»™i dung chÃ­nh (tá»‘i quan trá»ng)
                if doc.content:
                    content_lower = doc.content.lower()
                    for kw in keywords:
                        count = content_lower.count(kw)
                        if count > 0:
                            score += count * 20  # TÄƒng Ä‘iá»ƒm sá»‘ cho ná»™i dung

                            # Láº¥y táº¥t cáº£ cÃ¡c Ä‘oáº¡n khá»›p
                            start_idx = 0
                            match_count = 0
                            while match_count < 5:  # Giá»›i háº¡n 5 Ä‘oáº¡n má»—i tá»« khÃ³a
                                idx = content_lower.find(kw, start_idx)
                                if idx == -1:
                                    break

                                snippet_start = max(0, idx - 100)
                                snippet_end = min(len(doc.content), idx + len(kw) + 100)
                                snippet = doc.content[snippet_start:snippet_end]

                                matches.append({
                                    'snippet': snippet.strip(),
                                    'chunk_index': match_count,
                                    'keyword': kw,
                                    'position': idx
                                })

                                start_idx = idx + len(kw)
                                match_count += 1

                # 3. TÃ¬m kiáº¿m trong file Ä‘Ã­nh kÃ¨m
                for att in doc.attachments:
                    if att.metadata and att.metadata.get('content'):
                        att_content = att.metadata['content'].lower()
                        for kw in keywords:
                            if kw in att_content:
                                score += 30  # Äiá»ƒm cho attachment
                                matches.append({
                                    'snippet': f"[Tá»« file Ä‘Ã­nh kÃ¨m: {att.filename}]",
                                    'source': 'attachment',
                                    'filename': att.filename,
                                    'keyword': kw
                                })

                # 4. TÃ¬m kiáº¿m trong metadata
                if doc.metadata:
                    metadata_str = str(doc.metadata).lower()
                    for kw in keywords:
                        if kw in metadata_str:
                            score += 15

                # 5. TÃ¬m kiáº¿m trong sá»‘ cÃ´ng vÄƒn
                if doc.document_number and any(kw in doc.document_number.lower() for kw in keywords):
                    score += 80

                # 6. TÃ¬m kiáº¿m trong ngÆ°á»i gá»­i
                if doc.sender and any(kw in doc.sender.lower() for kw in keywords):
                    score += 40

                if score > 0:
                    # Táº¡o ná»™i dung vá»›i highlight
                    highlighted = doc.content if doc.content else ""
                    for kw in keywords:
                        highlighted = highlighted.replace(
                            kw,
                            f"<mark>{kw}</mark>"
                        )

                    results.append({
                        'document': doc,
                        'score': score,
                        'matches': matches[:10],  # Giá»›i háº¡n 10 Ä‘oáº¡n má»—i tÃ i liá»‡u
                        'highlighted_content': highlighted[:500] if highlighted else ""
                    })

            # Sáº¯p xáº¿p theo Ä‘iá»ƒm sá»‘
            results = sorted(results, key=lambda x: x['score'], reverse=True)[:limit]
            return results

        except Exception as e:
            print(f"[ERROR] Enhanced search error: {str(e)}")
            return []

    # ============ PROCESS CHAT MESSAGE ============

    @staticmethod
    def process_chat_message(message: str) -> Tuple[str, List[Any]]:
        """
        Xá»­ lÃ½ tin nháº¯n chat tá»« ngÆ°á»i dÃ¹ng

        Args:
            message: Tin nháº¯n tá»« ngÆ°á»i dÃ¹ng

        Returns:
            Tuple[response, related_documents]
        """
        try:
            message_lower = message.lower()

            # Nháº­n diá»‡n Ã½ Ä‘á»‹nh ngÆ°á»i dÃ¹ng
            intent = AIService._detect_intent(message_lower)

            if intent == 'search':
                # TrÃ­ch xuáº¥t tá»« khÃ³a tÃ¬m kiáº¿m
                keywords = AIService._extract_keywords(message)
                results = AIService.search_documents(keywords, limit=5)

                response = f"TÃ´i tÃ¬m tháº¥y {len(results)} káº¿t quáº£ liÃªn quan Ä‘áº¿n '{keywords}':\n\n"

                for i, result in enumerate(results, 1):
                    doc = result['document']
                    response += f"{i}. {doc.title} "
                    if doc.document_number:
                        response += f"(Sá»‘: {doc.document_number})"
                    response += "\n"

                return response, [r['document'] for r in results]

            elif intent == 'statistics':
                from models import Document

                total_docs = Document.query.count()
                today = datetime.utcnow().date()
                today_docs = Document.query.filter(
                    Document.created_at >= datetime(today.year, today.month, today.day)
                ).count()

                response = f"ğŸ“Š Thá»‘ng kÃª há»‡ thá»‘ng:\n"
                response += f"â€¢ Tá»•ng sá»‘ vÄƒn báº£n: {total_docs}\n"
                response += f"â€¢ VÄƒn báº£n hÃ´m nay: {today_docs}\n"
                response += f"â€¢ CÃ´ng vÄƒn: {Document.query.filter_by(document_type='CÃ´ng vÄƒn').count()}\n"
                response += f"â€¢ Quyáº¿t Ä‘á»‹nh: {Document.query.filter_by(document_type='Quyáº¿t Ä‘á»‹nh').count()}"

                return response, []

            elif intent == 'list':
                from models import Document

                docs = Document.query.order_by(Document.created_at.desc()).limit(5).all()

                response = "ğŸ“‹ CÃ¡c vÄƒn báº£n gáº§n Ä‘Ã¢y:\n\n"
                for i, doc in enumerate(docs, 1):
                    response += f"{i}. {doc.title}\n"
                    if doc.sender:
                        response += f"   Tá»«: {doc.sender}\n"
                    if doc.date_received:
                        response += f"   NgÃ y: {doc.date_received.strftime('%d/%m/%Y')}\n"
                    response += "\n"

                return response, docs

            elif intent == 'help':
                response = """ğŸ’¡ TÃ´i cÃ³ thá»ƒ giÃºp báº¡n:

â€¢ "TÃ¬m cÃ´ng vÄƒn vá»..." - TÃ¬m kiáº¿m vÄƒn báº£n
â€¢ "Thá»‘ng kÃª" - Xem thá»‘ng kÃª há»‡ thá»‘ng
â€¢ "Danh sÃ¡ch" - Xem cÃ¡c vÄƒn báº£n gáº§n Ä‘Ã¢y
â€¢ "TÃ¬m kiáº¿m nÃ¢ng cao" - TÃ¬m kiáº¿m chi tiáº¿t
â€¢ "Sá»‘ lÆ°á»£ng" - Äáº¿m sá»‘ vÄƒn báº£n

Báº¡n cÅ©ng cÃ³ thá»ƒ Ä‘áº·t báº¥t ká»³ cÃ¢u há»i nÃ o vá» cÃ¡c vÄƒn báº£n!"""
                return response, []

            else:
                # Tráº£ lá»i chung chung
                response = f"Xin lá»—i, tÃ´i khÃ´ng hiá»ƒu rÃµ: '{message}'\n\n"
                response += "Vui lÃ²ng thá»­:\n"
                response += "â€¢ 'TÃ¬m cÃ´ng vÄƒn vá»...' - Ä‘á»ƒ tÃ¬m kiáº¿m\n"
                response += "â€¢ 'Thá»‘ng kÃª' - Ä‘á»ƒ xem thá»‘ng kÃª\n"
                response += "â€¢ 'Danh sÃ¡ch' - Ä‘á»ƒ xem cÃ¡c vÄƒn báº£n gáº§n Ä‘Ã¢y"

                return response, []

        except Exception as e:
            print(f"[ERROR] Chat processing error: {str(e)}")
            return f"ÄÃ£ xáº£y ra lá»—i: {str(e)}", []

    # ============ INTENT DETECTION ============

    @staticmethod
    def _detect_intent(message: str) -> str:
        """
        Nháº­n diá»‡n Ã½ Ä‘á»‹nh tá»« tin nháº¯n

        Args:
            message: Tin nháº¯n (Ä‘Ã£ chuyá»ƒn thÃ nh lowercase)

        Returns:
            Intent type (search, statistics, list, help, general)
        """
        search_keywords = ['tÃ¬m', 'tÃ¬m kiáº¿m', 'search', 'liÃªn quan']
        stats_keywords = ['thá»‘ng kÃª', 'statistics', 'sá»‘ lÆ°á»£ng', 'tá»•ng', 'bao nhiÃªu']
        list_keywords = ['danh sÃ¡ch', 'list', 'xem', 'gáº§n Ä‘Ã¢y', 'má»›i nháº¥t']
        help_keywords = ['giÃºp', 'help', 'lÃ m gÃ¬', 'cÃ³ thá»ƒ gÃ¬', 'há»i']

        if any(kw in message for kw in search_keywords):
            return 'search'
        elif any(kw in message for kw in stats_keywords):
            return 'statistics'
        elif any(kw in message for kw in list_keywords):
            return 'list'
        elif any(kw in message for kw in help_keywords):
            return 'help'

        return 'general'

    # ============ KEYWORD EXTRACTION ============

    @staticmethod
    def _extract_keywords(message: str) -> str:
        """
        TrÃ­ch xuáº¥t tá»« khÃ³a tá»« tin nháº¯n

        Args:
            message: Tin nháº¯n tá»« ngÆ°á»i dÃ¹ng

        Returns:
            Chuá»—i tá»« khÃ³a
        """
        stopwords = ['tÃ¬m', 'tÃ¬m kiáº¿m', 'search', 'cÃ´ng', 'vÄƒn', 'vá»', 'liÃªn quan', 'cÃ³', 'lÃ ']

        words = message.lower().split()
        keywords = [w for w in words if w not in stopwords and len(w) > 2]

        return ' '.join(keywords) if keywords else message

    # ============ DOCUMENT ANALYSIS ============

    @staticmethod
    def analyze_document_sentiment(content: str) -> Dict[str, Any]:
        """
        PhÃ¢n tÃ­ch cáº£m xÃºc/tÃ­nh cháº¥t cá»§a tÃ i liá»‡u

        Args:
            content: Ná»™i dung tÃ i liá»‡u

        Returns:
            Dict vá»›i sentiment, score vÃ  indicators
        """
        if not content:
            return {'sentiment': 'neutral', 'score': 0}

        positive_words = ['quan trá»ng', 'kháº©n', 'cáº§n thiáº¿t', 'Æ°u tiÃªn', 'thÃ nh cÃ´ng', 'hoÃ n thÃ nh']
        negative_words = ['há»§y', 'tá»« chá»‘i', 'táº¡m dá»«ng', 'lá»—i', 'váº¥n Ä‘á»']

        content_lower = content.lower()
        positive_count = sum(1 for word in positive_words if word in content_lower)
        negative_count = sum(1 for word in negative_words if word in content_lower)

        if positive_count > negative_count:
            sentiment = 'positive'
            score = min(positive_count / 10, 1.0)
        elif negative_count > positive_count:
            sentiment = 'negative'
            score = -min(negative_count / 10, 1.0)
        else:
            sentiment = 'neutral'
            score = 0

        return {
            'sentiment': sentiment,
            'score': score,
            'positive_indicators': positive_count,
            'negative_indicators': negative_count
        }

    # ============ ENTITY EXTRACTION ============

    @staticmethod
    def extract_entities(text: str) -> Dict[str, Any]:
        """
        TrÃ­ch xuáº¥t cÃ¡c thá»±c thá»ƒ tá»« vÄƒn báº£n (ngÆ°á»i, tá»• chá»©c, ngÃ y thÃ¡ng)

        Args:
            text: VÄƒn báº£n input

        Returns:
            Dict chá»©a dates, organizations, numbers
        """
        entities = {
            'dates': [],
            'organizations': [],
            'numbers': []
        }

        try:
            # TÃ¬m ngÃ y thÃ¡ng
            date_pattern = r'\d{1,2}[/-]\d{1,2}[/-]\d{2,4}'
            entities['dates'] = re.findall(date_pattern, text)

            # TÃ¬m sá»‘
            number_pattern = r'\d+(?:\.\d+)?'
            entities['numbers'] = re.findall(number_pattern, text)

            # TÃ¬m tá»• chá»©c
            org_pattern = r'\b[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*\b'
            entities['organizations'] = list(set(re.findall(org_pattern, text)))[:10]

        except Exception as e:
            print(f"[ERROR] Entity extraction error: {str(e)}")

        return entities

    # ============ DOCUMENT SUMMARIZATION ============

    @staticmethod
    def generate_summary(text: str, max_length: int = 200) -> str:
        """
        Táº¡o tÃ³m táº¯t tá»« vÄƒn báº£n

        Args:
            text: VÄƒn báº£n input
            max_length: Äá»™ dÃ i tá»‘i Ä‘a cá»§a tÃ³m táº¯t

        Returns:
            TÃ³m táº¯t vÄƒn báº£n
        """
        if not text or len(text) < max_length:
            return text

        sentences = text.split('.')
        summary_sentences = []
        total_length = 0

        for sentence in sentences:
            if total_length < max_length:
                summary_sentences.append(sentence.strip())
                total_length += len(sentence)
            else:
                break

        return '. '.join(summary_sentences) + '...'


# Export AIService
__all__ = ['AIService']